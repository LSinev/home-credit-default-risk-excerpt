{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ca25a2f95f0fc9498b9b2e3a9d96607fbb682015"
   },
   "source": [
    "# Home Credit Default Risk 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cc4088625ae2209899d05c70dfd7bcb108cb4c3a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.simplefilter(action = 'ignore', category = FutureWarning)\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 142\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../input/'\n",
    "source_path = '../stack_it/stack_source/'\n",
    "result_path = '../stack_it/stacking_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv(f'{file_path}sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "be294207f4e12ccf54d922814789b27998577dca"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "962799c45d0c88ee9237cb0e202b11758abac536"
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "42960cc0257a40fee0876294c6956e0aeba23023"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b5e7cfa7c94294ad8b1eab00438592d24450e11"
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3687d746e5fe89d3477cff12521e1402431c7ab1"
   },
   "source": [
    "### Service functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4b1d9d1291ce3da6dc089ec6c3918f485528caf0"
   },
   "outputs": [],
   "source": [
    "def convert_col_to_proper_int(df_col):\n",
    "    col_type = df_col.dtype\n",
    "#     print('convert_col_to_proper_int column: ', df_col.name, 'type: ', col_type, 'c_min: ', c_min)\n",
    "    if ((str(col_type)[:3] == 'int') | (str(col_type)[:4] == 'uint')): # | (str(col_type)[:5] == 'float')\n",
    "        c_min = df_col.min()\n",
    "        c_max = df_col.max()\n",
    "        if c_min < 0:\n",
    "#             print('c_min: ', c_min, 'less 0')\n",
    "            if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "                df_col = df_col.astype(np.int8)\n",
    "            elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                df_col = df_col.astype(np.int16)\n",
    "            elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                df_col = df_col.astype(np.int32)\n",
    "            elif c_min >= np.iinfo(np.int64).min and c_max <= np.iinfo(np.int64).max:\n",
    "                df_col = df_col.astype(np.int64)\n",
    "        else:\n",
    "#             print('c_min: ', c_min, 'not less 0')\n",
    "            if c_max <= np.iinfo(np.uint8).max:\n",
    "                df_col = df_col.astype(np.uint8)\n",
    "            elif c_max <= np.iinfo(np.uint16).max:\n",
    "                df_col = df_col.astype(np.uint16)\n",
    "            elif c_max <= np.iinfo(np.uint32).max:\n",
    "                df_col = df_col.astype(np.uint32)\n",
    "            elif c_max <= np.iinfo(np.uint64).max:\n",
    "                df_col = df_col.astype(np.uint64)\n",
    "            \n",
    "    return df_col\n",
    "\n",
    "def convert_col_to_proper_float(df_col):\n",
    "    col_type = df_col.dtype\n",
    "    if str(col_type)[:5] == 'float':\n",
    "        unique_count = len(np.unique(df_col))\n",
    "        df_col_temp = df_col.astype(np.float32)\n",
    "        if len(np.unique(df_col_temp)) == unique_count:\n",
    "            df_col = df_col_temp\n",
    "            c_min = df_col.min()\n",
    "            c_max = df_col.max()\n",
    "            if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                df_col_temp = df_col.astype(np.float16)\n",
    "                if len(np.unique(df_col_temp)) == unique_count:\n",
    "                    df_col = df_col_temp\n",
    "            \n",
    "    return df_col\n",
    "\n",
    "\n",
    "\n",
    "def float_to_int(df):\n",
    "    \"\"\" iterate through all float columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "#     print('Begin float_to_int')\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "#         print('column: ', col, 'type: ', col_type)\n",
    "        if str(col_type)[:5] == 'float':\n",
    "            if (df[col] % 1 == 0).all():\n",
    "                df[col] = convert_col_to_proper_int(df[col].astype(np.int64))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def float_reduced(df):\n",
    "    \"\"\" iterate through all float columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "#     print('Begin float_reduced')\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "#         print('column: ', col, 'type: ', col_type)\n",
    "        if str(col_type)[:5] == 'float':\n",
    "            df[col] = convert_col_to_proper_float(df[col])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def int_reduced(df):\n",
    "    \"\"\" iterate through all int columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "#     print('Begin float_reduced')\n",
    "    for col in df.columns:\n",
    "        df[col] = convert_col_to_proper_int(df[col])\n",
    "    \n",
    "    return df\n",
    "\n",
    "## Thanks You Guillaume Martin for the Awesome Memory Optimizer!\n",
    "## https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "def reduce_mem_usage(data, verbose = True):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = data.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Memory usage of dataframe: {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in data.columns:\n",
    "#         print(col, type(data[col]), data[col].shape)\n",
    "        col_type = data[col].dtype\n",
    "\n",
    "        if ((col_type != object) & (col_type != '<M8[ns]') & (col_type.name != 'category')):#\n",
    "            c_min = data[col].min()\n",
    "            c_max = data[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                data[col] = convert_col_to_proper_int(data[col])\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    data[col] = data[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    data[col] = data[col].astype(np.float32)\n",
    "                else:\n",
    "                    data[col] = data[col].astype(np.float64)\n",
    "        else: data[col] = data[col].astype('category')\n",
    "\n",
    "    end_mem = data.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Memory usage after optimization: {:.2f} MB'.format(end_mem))\n",
    "        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return data\n",
    "\n",
    "def gentle_reduce_mem_usage(data, verbose = True):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = data.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Memory usage of dataframe: {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in data.columns:\n",
    "#         print(col, type(data[col]), data[col].shape)\n",
    "        col_type = data[col].dtype\n",
    "\n",
    "        if ((col_type != object) & (col_type != '<M8[ns]') & (col_type.name != 'category')):#\n",
    "            c_min = data[col].min()\n",
    "            c_max = data[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                data[col] = convert_col_to_proper_int(data[col])\n",
    "            else:\n",
    "                if (data[col] % 1 == 0).all():\n",
    "                    data[col] = convert_col_to_proper_int(data[col].astype(np.int64))\n",
    "                else:\n",
    "                    data[col] = convert_col_to_proper_float(data[col])\n",
    "        else: data[col] = data[col].astype('category')\n",
    "\n",
    "    end_mem = data.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Memory usage after optimization: {:.2f} MB'.format(end_mem))\n",
    "        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_pickle(f'{file_path}models_df07.pkl.zip')\n",
    "df.reset_index(inplace = True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['SK_ID_CURR', 'TARGET',\n",
    " 'konyshev_0775CV',\n",
    " 'ppleskov_neptune_LB0802',\n",
    "#  'leonid26_638feats_5CV_078196',\n",
    " 'tEarth_pred10_CV0794662',\n",
    "#  'leonid22_719feats_5CV_079012',\n",
    " 'tEarth_pred11_LB0798',\n",
    "#  'leonid08_5CV_078551',\n",
    "#  'leonid16_1293feats_5CV_079173',\n",
    "#  'leonid21_315feats_5CV_079244',\n",
    "#  'leonid24_1350feats_5CV_079194',\n",
    "#  'leonid23_285feats_5CV_079546',\n",
    "#  'leonid13_5CV_079019',\n",
    " 'iv_079690CV_xxxxPL',\n",
    " 'iv_079581CV_0795PL',\n",
    "#  'leonid12_5CV_079137',\n",
    "#  'leonid17_1155feats_5CV_079172',\n",
    " 'iv_079634CV_0794PL']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6f7495063700c9e611ea1e27f90f40acd5085cae"
   },
   "source": [
    "## Optimization LGBM parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9148f2a806be7e22a5cfb90442d82a72e1da4084"
   },
   "source": [
    "### Optimization and visualisation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_average_df(data):\n",
    "    for key in data.columns:\n",
    "        data[str(key) + '_rank'] = data[key].rank()\n",
    "    data['rank_sum'] = np.sum(data[col] for col in data.columns if '_rank' in str(col))\n",
    "#     print(data.shape[0], len([col for col in data.columns if '_rank' in str(col)]))\n",
    "    data['TARGET'] = data['rank_sum']/(len([col for col in data.columns if '_rank' in str(col)]) *\n",
    "            data.shape[0])\n",
    "    return data['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b440750e95bc79ad0603245387b08aefe048b0af"
   },
   "outputs": [],
   "source": [
    "def cv_scores(df, num_folds, params, model_name, stratified = False, verbose = -1,\n",
    "              early_stopping = 300,\n",
    "              save_train_prediction = True, save_test_prediction = True, train_full_model=False,\n",
    "              folder_to_save='../stack_it/', seed = 42, submission_sample = submission_df):\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "#     clf = LGBMClassifier(class_weight = 'balanced', importance_type = 'gain',\n",
    "#                          random_state = RANDOM_STATE, **params)\n",
    "\n",
    "    # Divide in training/validation and test data\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    test_df = df[df['TARGET'].isnull()]\n",
    "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits = num_folds, shuffle = True, random_state = seed)\n",
    "    else:\n",
    "        folds = KFold(n_splits = num_folds, shuffle = True, random_state = seed)\n",
    "        \n",
    "    # Create arrays and dataframes to store results\n",
    "#     train_pred = np.zeros(train_df.shape[0])\n",
    "#     train_pred_proba = np.zeros(train_df.shape[0])\n",
    "\n",
    "#     valid_pred = np.zeros(train_df.shape[0])\n",
    "    valid_pred_proba = np.zeros(train_df.shape[0])\n",
    "    \n",
    "#     test_prediction = np.zeros(test_df.shape[0])\n",
    "    test_prediction = pd.DataFrame(index = submission_sample['SK_ID_CURR'])\n",
    "    \n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    df_feature_importance = pd.DataFrame(index = feats)\n",
    "    \n",
    "    iterations = np.zeros(num_folds, dtype=np.uint16)\n",
    "    train_scores = np.zeros(num_folds, dtype=np.float32)\n",
    "    fold_scores = np.zeros(num_folds, dtype=np.float32)\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        clf = LGBMClassifier(importance_type = 'gain',# class_weight = 'balanced',\n",
    "                             random_state = (RANDOM_STATE + n_fold), **params)\n",
    "        print('Fold', n_fold, 'started at', time.ctime())\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "\n",
    "        clf.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)], eval_metric = 'auc',\n",
    "                eval_names = [f'{n_fold}_train', f'{n_fold}_valid'],\n",
    "                verbose = verbose, early_stopping_rounds = early_stopping)\n",
    "        iterations[n_fold] = clf.best_iteration_\n",
    "\n",
    "#         train_pred[train_idx] = clf.predict(train_x, num_iteration = clf.best_iteration_)\n",
    "#         train_pred_proba[train_idx] = clf.predict_proba(train_x, num_iteration = clf.best_iteration_)[:, 1]\n",
    "#         valid_pred[valid_idx] = clf.predict(valid_x, num_iteration = clf.best_iteration_)\n",
    "        valid_pred_proba[valid_idx] = clf.predict_proba(valid_x, num_iteration = clf.best_iteration_)[:, 1]\n",
    "        \n",
    "#         test_prediction += \\\n",
    "#                 clf.predict_proba(test_df[feats], num_iteration = clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "        test_prediction[n_fold] = pd.Series(clf.predict_proba(test_df[feats], num_iteration = clf.best_iteration_)[:, 1],\n",
    "                                            index = submission_sample['SK_ID_CURR'])\n",
    "\n",
    "        df_feature_importance[n_fold] = pd.Series(clf.feature_importances_, index = feats)\n",
    "#         print(clf.booster_.attr('max_depth'))\n",
    "        train_scores[n_fold] = clf.evals_result_[f'{n_fold}_train']['auc'][clf.best_iteration_-1]\n",
    "        fold_scores[n_fold] = roc_auc_score(valid_y, valid_pred_proba[valid_idx])\n",
    "        print('Fold %2d train AUC: %.6f, valid AUC: %.6f; best iteration %5d;' % (n_fold,\n",
    "                                                                                  train_scores[n_fold],\n",
    "                                                                                  fold_scores[n_fold],\n",
    "                                                                                  iterations[n_fold]))\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "#     roc_auc_train = roc_auc_score(train_df['TARGET'], train_pred_proba)\n",
    "#     precision_train = precision_score(train_df['TARGET'], train_pred, average = None)\n",
    "#     recall_train = recall_score(train_df['TARGET'], train_pred, average = None)\n",
    "    \n",
    "    roc_auc_valid = roc_auc_score(train_df['TARGET'], valid_pred_proba)\n",
    "#     precision_valid = precision_score(train_df['TARGET'], valid_pred, average = None)\n",
    "#     recall_valid = recall_score(train_df['TARGET'], valid_pred, average = None)\n",
    "\n",
    "    print('Full AUC score %.6f' % roc_auc_valid)\n",
    "    print(f'AUC score across {num_folds} folds is mean {fold_scores.mean():.4f} with std {fold_scores.std():.4f}',\n",
    "          f'(train score mean {train_scores.mean():.4f} with std {train_scores.std():.4f})')\n",
    "    \n",
    "    df_feature_importance.fillna(0, inplace = True)\n",
    "    df_feature_importance['mean'] = df_feature_importance.mean(axis = 1)\n",
    "    important_feats = df_feature_importance[df_feature_importance['mean']>0]\\\n",
    "                        .sort_values(by='mean', ascending = False).index.tolist()\n",
    "\n",
    "    model_description = f'{model_name}_{len(feats)}feats_{num_folds}CV_{roc_auc_valid*1e5:0>6.0f}'\n",
    "    # Write prediction files\n",
    "    if save_train_prediction:\n",
    "#         df_prediction = train_df[['SK_ID_CURR', 'TARGET']]\n",
    "        df_prediction = pd.DataFrame({'SK_ID_CURR': train_df['SK_ID_CURR'].values,\n",
    "                                      'TARGET': valid_pred_proba})\n",
    "        train_prediction_file_name = f'{folder_to_save}{model_description}_oof_train.csv'\n",
    "        print(f'Saving oof train predictions to {train_prediction_file_name}')\n",
    "        df_prediction.to_csv(train_prediction_file_name, index = False)\n",
    "        del df_prediction\n",
    "        gc.collect()\n",
    "\n",
    "    if save_test_prediction:\n",
    "        test_predictions_file_name = f'{folder_to_save}{model_description}_{num_folds}_test_preds.csv'\n",
    "        print(f'Saving {num_folds} folds test predictions to {test_predictions_file_name}')\n",
    "        test_prediction.to_csv(test_predictions_file_name, index = False)\n",
    "        \n",
    "        df_prediction = pd.DataFrame({'SK_ID_CURR': submission_sample['SK_ID_CURR'].values,\n",
    "                                      'TARGET': test_prediction.mean(axis = 1)})\n",
    "        test_mean_prediction_file_name = f'{folder_to_save}{model_description}_test_mean.csv'\n",
    "        print(f'Saving mean of {num_folds} test predictions to {test_mean_prediction_file_name}')\n",
    "        df_prediction.to_csv(test_mean_prediction_file_name, index = False)\n",
    "\n",
    "        df_prediction = pd.DataFrame({'SK_ID_CURR': submission_sample['SK_ID_CURR'].values,\n",
    "                                      'TARGET': gmean(test_prediction, axis=1)})\n",
    "        test_gmean_prediction_file_name = f'{folder_to_save}{model_description}_test_gmean.csv'\n",
    "        print(f'Saving geometric mean of {num_folds} test predictions to {test_gmean_prediction_file_name}')\n",
    "        df_prediction.to_csv(test_gmean_prediction_file_name, index = False)\n",
    "       \n",
    "        df_prediction = pd.DataFrame({'SK_ID_CURR': submission_sample['SK_ID_CURR'].values,\n",
    "                                      'TARGET': rank_average_df(test_prediction)})\n",
    "        test_ranked_prediction_file_name = f'{folder_to_save}{model_description}_test_rank_averaged.csv'\n",
    "        print(f'Saving rank average of {num_folds} test predictions to {test_ranked_prediction_file_name}')\n",
    "        df_prediction.to_csv(test_ranked_prediction_file_name, index = False)\n",
    "\n",
    "        del df_prediction\n",
    "        gc.collect()\n",
    "    \n",
    "    if train_full_model:\n",
    "        print(f'Full train learning for {iterations.max()} iterations',\n",
    "              f'on {len(important_feats)} features of original {len(feats)}',\n",
    "              'started at', time.ctime())\n",
    "        train_x, train_y = train_df[important_feats], train_df['TARGET']\n",
    "        params['n_estimators'] = iterations.max()\n",
    "        clf_full = LGBMClassifier(importance_type = 'gain', #class_weight = 'balanced',\n",
    "                                 random_state = RANDOM_STATE, **params)\n",
    "        clf_full.fit(train_x, train_y,\n",
    "                verbose = verbose)\n",
    "\n",
    "\n",
    "        fulltrained_model_description = f'{model_name}_{len(important_feats)}feats_{num_folds}CV_{roc_auc_valid*1e5:0>6.0f}'\n",
    "        if save_test_prediction:\n",
    "    #         df_prediction = test_df[['SK_ID_CURR', 'TARGET']]\n",
    "            test_full_prediction_file_name = f'{folder_to_save}{fulltrained_model_description}_test_fulltrained.csv'\n",
    "            print(f'Saving fulltrained test predictions to {test_full_prediction_file_name}')\n",
    "            df_prediction = pd.DataFrame({'SK_ID_CURR': submission_sample['SK_ID_CURR'].values,\n",
    "                                          'TARGET': clf_full.predict_proba(test_df[important_feats])[:, 1]})\n",
    "\n",
    "    #         df_prediction['TARGET'] = pd.Series(clf_full.predict_proba(test_df[important_feats])[:, 1],\n",
    "    #                                             index = test_df['SK_ID_CURR'])\n",
    "            df_prediction.to_csv(test_full_prediction_file_name, index = False)\n",
    "\n",
    "            del df_prediction\n",
    "            gc.collect()\n",
    "    \n",
    "    return df_feature_importance, \\\n",
    "            roc_auc_valid\n",
    "#            [#roc_auc_train,\n",
    "#             roc_auc_valid,\n",
    "#             precision_train[0], precision_valid[0], precision_train[1], precision_test[1],\n",
    "#             recall_train[0], recall_test[0], recall_train[1], recall_test[1], 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe4fb091c39f4edd0bbf997bcc5630e35b47228b"
   },
   "source": [
    "### LightGBM stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a0c18d584ffb1abbc944ae5f2f78e9b30b67c3f7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lgbm_params = {\n",
    "            'objective': 'binary',\n",
    "            'nthread': 4,\n",
    "            'n_estimators': 10000,\n",
    "            'max_depth': -1,\n",
    "            'boosting_type': 'gbdt',#goss\n",
    "            'learning_rate': 0.005,#0.02\n",
    "            'num_leaves': 600,\n",
    "            'max_bin': 255,#255 #100\n",
    "            'colsample_bytree': 0.48,#0.05,#1.0\n",
    "            'reg_lambda': 600,#1000\n",
    "            'bagging_fraction': 0.35,#0.15\n",
    "            'bagging_freq': 15,\n",
    "            'min_data_in_leaf': 375,#70\n",
    "            'min_gain_to_split': 0.5,\n",
    "#             'reg_alpha': .041545473,\n",
    "#             'min_child_weight': 39.3259775,\n",
    "            'silent': -1,\n",
    "            'verbose': -1\n",
    "}\n",
    "\n",
    "\n",
    "feature_importance, scor = cv_scores(df, 5, lgbm_params, model_name = 'leonid40latestack',\n",
    "                                     save_train_prediction = True,\n",
    "                                     verbose = 200, early_stopping=1000, folder_to_save = result_path,\n",
    "                                     stratified = True, seed = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.sort_values('mean', ascending = False).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ecd58af84525839510d1669efb1cc0bb251e310"
   },
   "outputs": [],
   "source": [
    "feature_importance.sort_values('mean', ascending = False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7394bcdf79fc237f81aa13636f39783e42d84a41"
   },
   "outputs": [],
   "source": [
    "df[df['TARGET'].isnull()][[f for f in df.columns.tolist() if f not in ['SK_ID_CURR', 'TARGET']]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.DataFrame({})\n",
    "# for file in os.listdir('./blend/'):\n",
    "#     df = pd.read_csv('./blend/'+file)\n",
    "#     df = df.rename_axis({'target':file},axis=1)\n",
    "#     data = pd.concat([data, df], axis=1)\n",
    "# data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas.rpy.common as com\n",
    "# import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# # load the R package ISLR\n",
    "# infert = com.importr(\"ISLR\")\n",
    "\n",
    "# # load the Auto dataset\n",
    "# auto_df = com.load_data('Auto')\n",
    "\n",
    "# calculate the correlation matrix\n",
    "# corr = auto_df.corr()\n",
    "corr = df[df['TARGET'].isnull()][[f for f in df.columns.tolist() if f not in ['SK_ID_CURR', 'TARGET']]].corr()\n",
    "\n",
    "# plot the heatmap\n",
    "# plt.figure(figsize=(16,9))\n",
    "sns.set(rc={'figure.figsize':(16,12)})\n",
    "sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df[df['TARGET'].notnull()][[f for f in df.columns.tolist() if f not in ['SK_ID_CURR', 'TARGET']]].corr()\n",
    "\n",
    "# plot the heatmap\n",
    "# plt.figure(figsize=(16,9))\n",
    "sns.set(rc={'figure.figsize':(16,12)})\n",
    "sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
